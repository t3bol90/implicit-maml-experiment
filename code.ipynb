{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d08e31e8-6204-4c51-8bfe-fed417ca1553",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Đoạn này import thư viện tổng\n",
    "from meta_cntk_utils import *\n",
    "from maml_utils import *\n",
    "from tqdm.notebook import trange\n",
    "import torch.nn.functional as F\n",
    "from time import time\n",
    "from copy import deepcopy\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "119b9d8e-2cff-47ae-a7fe-86963e291853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy scipy matplotlib pandas scikit-image scikit-learn higher torchmeta cupy-cuda111"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73ef2390-90ad-423f-8a66-64af56960c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Đoạn này cũng thư viện inner-level\n",
    "\"\"\"\n",
    "Meta-learning Omniglot and mini-imagenet experiments with iMAML-GD (see [1] for more details).\n",
    "\n",
    "The code is quite simple and easy to read thanks to the following two libraries which need both to be installed.\n",
    "- higher: https://github.com/facebookresearch/higher (used to get stateless version of torch nn.Module-s)\n",
    "- torchmeta: https://github.com/tristandeleu/pytorch-meta (used for meta-dataset loading and minibatching)\n",
    "\n",
    "\n",
    "[1] Rajeswaran, A., Finn, C., Kakade, S. M., & Levine, S. (2019).\n",
    "    Meta-learning with implicit gradients. In Advances in Neural Information Processing Systems (pp. 113-124).\n",
    "    https://arxiv.org/abs/1909.04630\n",
    "\"\"\"\n",
    "import math\n",
    "import argparse\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from torchmeta.datasets.helpers import omniglot, miniimagenet\n",
    "from torchmeta.utils.data import BatchMetaDataLoader\n",
    "\n",
    "import higher\n",
    "from maml_utils import np2torch\n",
    "import hypergrad as hg\n",
    "import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43d943c8-b7e1-41d3-aa65-59c28797c934",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Task:\n",
    "    \"\"\"\n",
    "    Handles the train and valdation loss for a single task\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, reg_param, meta_model, data, batch_size=None):\n",
    "        device = next(meta_model.parameters()).device\n",
    "\n",
    "        # stateless version of meta_model\n",
    "        self.fmodel = higher.monkeypatch(meta_model, device=device, copy_initial_weights=True)\n",
    "\n",
    "        self.n_params = len(list(meta_model.parameters()))\n",
    "        self.train_input, self.train_target, self.test_input, self.test_target = data\n",
    "        self.reg_param = reg_param\n",
    "        self.batch_size = 1 if not batch_size else batch_size\n",
    "        self.val_loss, self.val_acc = None, None\n",
    "\n",
    "    def bias_reg_f(self, bias, params):\n",
    "        # l2 biased regularization\n",
    "        return sum([((b - p) ** 2).sum() for b, p in zip(bias, params)])\n",
    "\n",
    "    def train_loss_f(self, params, hparams):\n",
    "        # biased regularized cross-entropy loss where the bias are the meta-parameters in hparams\n",
    "        out = self.fmodel(self.train_input, params=params)\n",
    "        return F.cross_entropy(out, self.train_target) + 0.5 * self.reg_param * self.bias_reg_f(hparams, params)\n",
    "\n",
    "    def val_loss_f(self, params, hparams):\n",
    "        # cross-entropy loss (uses only the task-specific weights in params\n",
    "        out = self.fmodel(self.test_input, params=params)\n",
    "        val_loss = F.cross_entropy(out, self.test_target)/self.batch_size\n",
    "        self.val_loss = val_loss.item()  # avoid memory leaks\n",
    "\n",
    "        pred = out.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "        self.val_acc = pred.eq(self.test_target.view_as(pred)).sum().item() / len(self.test_target)\n",
    "\n",
    "        return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9787419b-c715-49ba-9d99-2c27305cc4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inner_loop(hparams, params, optim, n_steps, log_interval, create_graph=False):\n",
    "    params_history = [optim.get_opt_params(params)]\n",
    "\n",
    "    for t in range(n_steps):\n",
    "        params_history.append(optim(params_history[-1], hparams, create_graph=create_graph))\n",
    "\n",
    "        if log_interval and (t % log_interval == 0 or t == n_steps-1):\n",
    "            print('t={}, Loss: {:.6f}'.format(t, optim.curr_loss.item()))\n",
    "\n",
    "    return params_history\n",
    "\n",
    "def get_cnn_omniglot(hidden_size, n_classes):\n",
    "    def conv_layer(ic, oc, ):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(ic, oc, 3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2),\n",
    "            nn.BatchNorm2d(oc, momentum=1., affine=True,\n",
    "                           track_running_stats=True\n",
    "                           )\n",
    "        )\n",
    "\n",
    "    net =  nn.Sequential(\n",
    "        conv_layer(1, hidden_size),\n",
    "        conv_layer(hidden_size, hidden_size),\n",
    "        conv_layer(hidden_size, hidden_size),\n",
    "        conv_layer(hidden_size, hidden_size),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(hidden_size, n_classes)\n",
    "    )\n",
    "\n",
    "    initialize(net)\n",
    "    return net\n",
    "\n",
    "\n",
    "def initialize(net):\n",
    "    for m in net.modules():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "            m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.zero_()\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            m.weight.data.fill_(1)\n",
    "            m.bias.data.zero_()\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            m.weight.data.zero_()\n",
    "            m.bias.data.zero_()\n",
    "\n",
    "    return net\n",
    "\n",
    "def get_inner_opt(train_loss):\n",
    "    inner_opt_class = hg.GradientDescent\n",
    "    inner_opt_kwargs = {'step_size': .1}\n",
    "    return inner_opt_class(train_loss, **inner_opt_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2cbcfafc-18ab-402a-a625-bd1e280c42bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_imaml(meta_model,db,reg_param,hg_mode,K,T,outer_opt,inner_log_interval,notebook=True):\n",
    "    meta_model.train()\n",
    "    n_train_iter = db.x_train.shape[0] // db.batchsz\n",
    "    for batch_idx in range(n_train_iter):\n",
    "        tr_xs, tr_ys, tst_xs, tst_ys = db.next()\n",
    "        outer_opt.zero_grad()\n",
    "\n",
    "        val_loss, val_acc = 0, 0\n",
    "        forward_time, backward_time = 0, 0\n",
    "        for t_idx, (tr_x, tr_y, tst_x, tst_y) in enumerate(zip(tr_xs, tr_ys, tst_xs, tst_ys)):\n",
    "            start_time_task = time.time()\n",
    "\n",
    "            # single task set up\n",
    "            task = Task(reg_param, meta_model, (tr_x, tr_y, tst_x, tst_y), batch_size=tr_xs.shape[0])\n",
    "            inner_opt = get_inner_opt(task.train_loss_f)\n",
    "\n",
    "            # single task inner loop\n",
    "            params = [p.detach().clone().requires_grad_(True) for p in meta_model.parameters()]\n",
    "            last_param = inner_loop(meta_model.parameters(), params, inner_opt, T, log_interval=inner_log_interval)[-1]\n",
    "            forward_time_task = time.time() - start_time_task\n",
    "\n",
    "            # single task hypergradient computation\n",
    "            if hg_mode == 'CG':\n",
    "                # This is the approximation used in the paper CG stands for conjugate gradient\n",
    "                cg_fp_map = hg.GradientDescent(loss_f=task.train_loss_f, step_size=1.)\n",
    "                hg.CG(last_param, list(meta_model.parameters()), K=K, fp_map=cg_fp_map, outer_loss=task.val_loss_f)\n",
    "            elif hg_mode == 'fixed_point':\n",
    "                hg.fixed_point(last_param, list(meta_model.parameters()), K=K, fp_map=inner_opt,\n",
    "                               outer_loss=task.val_loss_f)\n",
    "\n",
    "            backward_time_task = time.time() - start_time_task - forward_time_task\n",
    "\n",
    "            val_loss += task.val_loss\n",
    "            val_acc += task.val_acc / task.batch_size\n",
    "\n",
    "            forward_time += forward_time_task\n",
    "            backward_time += backward_time_task\n",
    "\n",
    "    outer_opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11cbbc1e-73d4-4309-9440-d52c16bf7cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_imaml(test_tasks, meta_model, n_steps, get_inner_opt, reg_param, log_interval=None):\n",
    "    meta_model.train()\n",
    "    device = next(meta_model.parameters()).device\n",
    "\n",
    "    val_losses, val_accs = [], []\n",
    "    tr_xs,tr_ys,tst_xs,tst_ys = test_tasks\n",
    "\n",
    "    tr_xs, tr_ys, tst_xs, tst_ys = np2torch([tr_xs, tr_ys, tst_xs, tst_ys],device=device,label_long_type=True)\n",
    "    for t_idx, (tr_x, tr_y, tst_x, tst_y) in enumerate(zip(tr_xs, tr_ys, tst_xs, tst_ys)):\n",
    "\n",
    "        task = Task(reg_param, meta_model, (tr_x, tr_y, tst_x, tst_y))\n",
    "        inner_opt = get_inner_opt(task.train_loss_f)\n",
    "\n",
    "        params = [p.detach().clone().requires_grad_(True) for p in meta_model.parameters()]\n",
    "        last_param = inner_loop(meta_model.parameters(), params, inner_opt, n_steps, log_interval=log_interval)[-1]\n",
    "\n",
    "        task.val_loss_f(last_param, meta_model.parameters())\n",
    "\n",
    "        val_losses.append(task.val_loss)\n",
    "        val_accs.append(task.val_acc)\n",
    "\n",
    "    return np.array(val_losses), np.array(val_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37e9b5f-5743-48d5-8e32-8e64885924a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d13a8780-050f-42ce-9289-90bfc5fbdd9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/imaml-exp/lib/python3.7/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at  ../c10/cuda/CUDAFunctions.cpp:115.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b03fbd8-048a-4705-835b-d80c75e650ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
    "seed = 42\n",
    "if torch.cuda.is_available():\n",
    "  device = torch.device(\"cuda\")\n",
    "else:\n",
    "  device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Success init\", device)\n",
    "\n",
    "torch.random.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b654877d-035e-47f4-b70a-a68c531c7397",
   "metadata": {},
   "outputs": [],
   "source": [
    "hg_mode = 'CG'\n",
    "inner_log_interval = None\n",
    "inner_log_interval_test = None\n",
    "ways = n_way = 5\n",
    "shots = n_shot = 5\n",
    "test_shots=20-n_shot\n",
    "\n",
    "n_channels = 64\n",
    "\n",
    "reg_param = 2\n",
    "T, K = 16, 5\n",
    "\n",
    "# K: number of CG step\n",
    "# T: number of innerloop update step\n",
    "\n",
    "T_test = T\n",
    "inner_lr = .1\n",
    "\n",
    "\n",
    "\n",
    "meta_model = get_cnn_omniglot(n_channels, ways).to(device)\n",
    "outer_opt = torch.optim.Adam(params=meta_model.parameters())\n",
    "inner_opt_class = hg.GradientDescent\n",
    "inner_opt_kwargs = {'step_size': inner_lr}\n",
    "import pickle\n",
    "def get_inner_opt(train_loss):\n",
    "    return inner_opt_class(train_loss, **inner_opt_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bd3fe6-9c16-4269-8580-9c8e575cf87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = {}\n",
    "n_characters = 20 # number of characters (i.e., classes) in the training dataset\n",
    "\n",
    "assert n_characters % n_way == 0 \n",
    "\n",
    "# Each task consists of n_way classes \n",
    "# So the least number of tasks is\n",
    "# n_characters//n_way.\n",
    "n_task = n_characters//n_way\n",
    "\n",
    "# number of channels for MAML CNN\n",
    "n_channel_maml = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb10482-10a9-46ec-8509-0bcd0cb5cd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Load raw dataset and encode label\n",
    "from types import SimpleNamespace\n",
    "import time\n",
    "dataset = load_dataset(n_task,True,seed)\n",
    "train_set,test_set = get_train_data(dataset,n_test_per_class=1)\n",
    "\n",
    "label_encoding_dim = 250\n",
    "batch_norm = True # batch_norm for the feature extractor\n",
    "dropout = 0 # Dropout for the feature extractor\n",
    "pretrain_batch_size = 16\n",
    "weight_decay = 0\n",
    "pretrain_epochs = 50\n",
    "train_data_enlarge_ratio =15\n",
    "# Construct a randomly initialized CNN as the feature extractor.\n",
    "net = build_CNN(train_set['n_class'], device, n_channel=label_encoding_dim,batch_norm=batch_norm,dropout=dropout)\n",
    "\n",
    "if pretrain_epochs > 0:\n",
    "    # Train a CNN on training data by supervised learning, in order\n",
    "    # to obtain a better feature extractor than a random CNN when\n",
    "    # training daorig_datasetta is relatively large. As the training data are of \n",
    "    # small-size, the supervised training leads to overfitted CNN,\n",
    "    # which is a worse feature extractor than a random CNN.\n",
    "    net,test_accs,test_losses = pretrain(net,train_set,test_set,device,seed=seed,epochs=pretrain_epochs,\n",
    "                                         weight_decay=weight_decay,batch_size=pretrain_batch_size)\n",
    "\n",
    "encode_labels(dataset,net,device)\n",
    "\n",
    "# Given n_way*n_task classes of samples, we can us resampling to obtain many tasks that consists of n_way distinct classes\n",
    "# of samples. The following is the resampling procedure.\n",
    "from copy import deepcopy\n",
    "orig_dataset = deepcopy(dataset)\n",
    "if train_data_enlarge_ratio > 1:\n",
    "    augment_train_data(dataset,enlarge_ratio=train_data_enlarge_ratio,n_way=n_way,n_shot=n_shot,seed=seed)\n",
    "\n",
    "preprocess_label_embeddings(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea83e411-0496-4640-b44c-ec5c40ae1d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Process data thành few-shot setting\n",
    "preprocess_label_embeddings(orig_dataset)\n",
    "tasks = vars(orig_dataset)\n",
    "\n",
    "X = np.concatenate([tasks['X_qry'],tasks['X_spt']],axis=1)\n",
    "Y = np.concatenate([tasks['Y_qry'],tasks['Y_spt']],axis=1)\n",
    "Y_emb = np.concatenate([tasks['Y_qry_emb'],tasks['Y_spt_emb']],axis=1)\n",
    "new_X =[]\n",
    "new_Y_emb = []\n",
    "for x,y,y_emb in zip(X,Y,Y_emb):\n",
    "    idxes = np.argsort(y).reshape(n_way,-1)\n",
    "    for i in range(idxes.shape[0]):\n",
    "        new_X.append([])\n",
    "        new_Y_emb.append([])\n",
    "        for j in range(idxes.shape[1]):\n",
    "            idx = idxes[i,j]\n",
    "            new_X[-1].append(x[idx])\n",
    "            new_Y_emb[-1].append(y_emb[idx])\n",
    "x_train = remove_padding(np.array(new_X))\n",
    "y_train = None\n",
    "\n",
    "test_tasks = remove_padding(tasks['test_X_spt']), tasks['test_Y_spt'],remove_padding(tasks['test_X_qry']), tasks['test_Y_qry']\n",
    "\n",
    "from support.omniglot_loaders_original import OmniglotNShot\n",
    "n_channel = n_channel_maml\n",
    "batchsz = min(32 if n_channel <= 1024 else 8, n_characters)\n",
    "\n",
    "db = OmniglotNShot(root=None,\n",
    "    batchsz=batchsz,\n",
    "    n_way=5,\n",
    "    k_shot=1,\n",
    "    k_query=19,\n",
    "    imgsz=28,\n",
    "    device=device,\n",
    "    n_train_tasks=None,\n",
    "    given_x=True,\n",
    "    x_train=x_train,\n",
    "    x_test=None,\n",
    "    y_train = y_train,\n",
    ")\n",
    "n_out = n_way\n",
    "net, meta_opt = build_MAML_model(n_out,device,lr=1e-3,n_channel=n_channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3891a394-454a-4e02-9f97-ab10a2b9732f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dtr_spx, Dtr_spy, Dtr_qrx, Dtr_qry = db.next()\n",
    "print(f\"Batchsize: {batchsz}\")\n",
    "print(f\"D train support set X shape: {Dtr_spx.shape}\")\n",
    "print(f\"D train support set Y shape: {Dtr_spy.shape}\")\n",
    "print(f\"D train query set X shape: {Dtr_qrx.shape}\")\n",
    "print(f\"D train query set Y shape: {Dtr_qry.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b63ff8-85aa-498a-a7f9-0a5b4d5d0540",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title iMAML\n",
    "\n",
    "from tqdm.notebook import trange\n",
    "import time\n",
    "\n",
    "imaml_epochs = 200\n",
    "eval_interval = 5\n",
    "log_interval = 5\n",
    "\n",
    "test_accs = []\n",
    "t = trange(imaml_epochs,desc='i-MAML Epoch')\n",
    "for k in t:\n",
    "    train_imaml(meta_model,db,reg_param,hg_mode,K,T,outer_opt,inner_log_interval)\n",
    "    if k % eval_interval == 0:\n",
    "        test_losses, test_acc = test_imaml(test_tasks, meta_model, T_test, get_inner_opt, reg_param, log_interval=None)\n",
    "        test_acc = np.mean(test_acc)*100\n",
    "        test_accs.append(test_acc)\n",
    "        t.set_postfix(test_acc=test_acc,max_test_acc=np.max(test_accs))\n",
    "accs['i-maml_accs'] = np.max(test_accs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
